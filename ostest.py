import os
import google.generativeai as genai
def api_caller(about):
 genai.configure(api_key=os.environ["GEMINI_API_KEY"])

# Create the model
 generation_config = {
  "temperature": 1,
  "top_p": 0.95,
  "top_k": 40,
  "max_output_tokens": 8192,
  "response_mime_type": "text/plain",
}

 model = genai.GenerativeModel(
  model_name="gemini-2.0-flash-exp",
  generation_config=generation_config,
  system_instruction="Prompts are instructions given to language models, these models then respond in accordance to those prompts, if prompted correctly they can perform correctly otherwise they sometimes lead to error, given instructions you can ask the model to give output in json and also of a particular type and it is always ensured that they stick to the format, I will give you example of a sample api call in python\n\"\"\"\nimport os\nimport google.generativeai as genai\nfrom google.ai.generativelanguage_v1beta.types import content\n\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n\n# Create the model\ngeneration_config = {\n  \"temperature\": 1,\n  \"top_p\": 0.95,\n  \"top_k\": 40,\n  \"max_output_tokens\": 8192,\n  \"response_schema\": content.Schema(\n    type = content.Type.OBJECT,\n    properties = {\n      \"Sample-array\": content.Schema(\n        type = content.Type.ARRAY,\n        items = content.Schema(\n          type = content.Type.OBJECT,\n          properties = {\n            \"nested_Value\": content.Schema(\n              type = content.Type.STRING,\n            ),\n          },\n        ),\n      ),\n      \"Integer_value\": content.Schema(\n        type = content.Type.INTEGER,\n      ),\n    },\n  ),\n  \"response_mime_type\": \"application/json\",\n}\n\nmodel = genai.GenerativeModel(\n  model_name=\"gemini-2.0-flash-exp\",\n  generation_config=generation_config,\n  system_instruction=\"These are instruction that the model must follow\",\n)\n\nchat_session = model.start_chat(\n  history=[\n  ]\n)\n\nresponse = chat_session.send_message(\"INSERT_INPUT_HERE\")\n\nprint(response.text)\n\"\"\"\nAs you can see we were also able to define a json schema for the output to follow, \nWhat I want you do is that I will give you a description of a function and I want you to take inspiration from this api and also ensure that you process the json output  \"response.text contains our json output\"",
 )

 chat_session = model.start_chat(
  history=[
    {
      "role": "user",
      "parts": [
        "{\n\"Func_name\": \"synthesis\",\n\"Implementation \": true,\n\"description\": \"This function takes two statements (a thesis and its antithesis) and generates a new statement that attempts to reconcile the two, creating a synthesis of the ideas. This process involves understanding the relationship between the thesis and antithesis, then creating a higher level more refined idea. The function is better suited for human/NLP tools because the relationship between statements is complex and involves reasoning. For example, synthesis('All cats are cute.','Some cats are not cute') may return 'Most cats are cute, but not all'.\"\n}",
      ],
    },
    {
      "role": "model",
      "parts": [
        "Okay, I understand. You want me to create a Python function that leverages a language model (like Gemini) to perform the \"synthesis\" task you described. I'll use the provided example API call as a guide and ensure we parse the JSON output appropriately.\n\nHere's a Python function that encapsulates this process:\n\n```python\nimport os\nimport google.generativeai as genai\nimport json\nfrom google.ai.generativelanguage_v1beta.types import content\n\ngenai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\")) # Use get to avoid KeyError\n\ndef synthesis(thesis: str, antithesis: str) -> str:\n    \"\"\"\n    Synthesizes a thesis and its antithesis into a new statement.\n\n    Args:\n        thesis: The initial statement.\n        antithesis: The opposing statement.\n\n    Returns:\n        A synthesized statement, or an error message if the synthesis fails.\n    \"\"\"\n\n    try:\n        # Define the generation config with the desired output schema\n        generation_config = {\n            \"temperature\": 0.7, # Adjusted temperature for more consistent synthesis\n            \"top_p\": 0.95,\n            \"top_k\": 40,\n            \"max_output_tokens\": 8192,\n             \"response_schema\": content.Schema(\n                type=content.Type.OBJECT,\n                properties={\n                    \"synthesis_statement\": content.Schema(\n                        type=content.Type.STRING,\n                    ),\n                },\n             ),\n            \"response_mime_type\": \"application/json\",\n\n        }\n\n        model = genai.GenerativeModel(\n            model_name=\"gemini-2.0-flash-exp\", # Or your preferred model\n            generation_config=generation_config,\n            system_instruction=\"You are an expert in generating synthesis between a thesis and antithesis statement\",\n        )\n\n        chat_session = model.start_chat(\n           history=[ ]\n        )\n        prompt = f\"Synthesize the following statements into a single, coherent statement:\\nThesis: {thesis}\\nAntithesis: {antithesis}\"\n        response = chat_session.send_message(prompt)\n\n        # Attempt to parse the JSON response\n        try:\n            json_output = json.loads(response.text)\n            return json_output.get(\"synthesis_statement\", \"Error: Could not extract synthesis statement from json.\")\n        except json.JSONDecodeError:\n            return f\"Error: Could not decode json. The response is: {response.text}\"\n\n    except Exception as e:\n      return f\"An error occurred: {e}\"\n\n\nif __name__ == '__main__':\n    thesis1 = \"All cats are cute.\"\n    antithesis1 = \"Some cats are not cute.\"\n    synthesis1 = synthesis(thesis1, antithesis1)\n    print(f\"Thesis: {thesis1}\\nAntithesis: {antithesis1}\\nSynthesis: {synthesis1}\\n\")\n    \n    thesis2 = \"The earth is flat\"\n    antithesis2 = \"The earth is spherical\"\n    synthesis2 = synthesis(thesis2, antithesis2)\n    print(f\"Thesis: {thesis2}\\nAntithesis: {antithesis2}\\nSynthesis: {synthesis2}\\n\")\n    \n    thesis3 = \"Artificial intelligence is harmless.\"\n    antithesis3 = \"Artificial intelligence can pose many risks.\"\n    synthesis3 = synthesis(thesis3, antithesis3)\n    print(f\"Thesis: {thesis3}\\nAntithesis: {antithesis3}\\nSynthesis: {synthesis3}\\n\")\n```\n\n**Explanation:**\n\n1.  **Import Necessary Libraries:**\n    *   `os` for environment variables.\n    *   `google.generativeai` for the Gemini API.\n    *   `json` for parsing JSON responses.\n\n2.  **`synthesis(thesis: str, antithesis: str) -> str` Function:**\n    *   Takes two strings, `thesis` and `antithesis`, as input.\n    *   **Error Handling:** It uses a `try-except` block to catch potential errors during the API call or JSON parsing.\n    *   **`generation_config`:**\n        *   Defines parameters for the language model's output.\n        *  Most importantly it contains the JSON schema to be followed `response_schema`.\n    *   **Gemini Model Initialization:**\n        *   Initializes the `gemini-2.0-flash-exp` model with the given configurations and prompt.\n    *   **Prompt Creation:**\n        *   Creates a clear and informative prompt for the model including thesis and antithesis statement.\n    *   **API Call and Response Handling:**\n        *   Sends the prompt to the model using `chat_session.send_message()`.\n        *   Attempts to parse the JSON response:\n            *   Uses `json.loads(response.text)` to parse the JSON.\n            *   Extracts the synthesis string using `json_output.get(\"synthesis_statement\", \"Error: Could not extract synthesis statement.\")`.\n        *   Returns an error message if JSON decoding fails.\n    *  **Exception handling:** handles any other exception during the synthesis process\n3.  **Example Usage (`if __name__ == '__main__':`)**\n    *   Demonstrates how to use the `synthesis` function with example thesis/antithesis pairs.\n\n**Key Improvements:**\n\n*   **JSON Schema:** The `response_schema` is defined in the  `generation_config` which constrains the model to output json with a particular format.\n*   **JSON Parsing:** The `json.loads()` function is used to properly parse the JSON response, and error handling was put in place for JSON decode errors.\n*   **Error Handling:**  Error handling (`try-except`) is included throughout the function, providing informative messages if something goes wrong.\n*   **Clear Return Values:** Returns the synthesis statement as a string or an error string if something goes wrong.\n*   **Clear Documentation:**  The function includes clear docstrings to explain its purpose and parameters.\n\n**To Use This Code:**\n\n1.  **Install the Gemini API Library:**\n    ```bash\n    pip install google-generativeai\n    ```\n2.  **Set Your API Key:**\n    *   Make sure you have your Gemini API key.\n    *   Set the `GEMINI_API_KEY` environment variable using:\n        ```bash\n        export GEMINI_API_KEY=\"YOUR_API_KEY\" # On Linux/macOS\n        set GEMINI_API_KEY=\"YOUR_API_KEY\"    # On Windows\n        ```\n        or you can use the method used in the example `os.environ.get(\"GEMINI_API_KEY\")`\n3.  **Run the Python Script:**\n    ```bash\n    python your_script_name.py\n    ```\n\nThis enhanced version provides a more robust and reliable way to use the Gemini model for synthesis tasks, incorporating proper JSON handling and error management. Let me know if you have further questions or want to customize it further.\n",
      ],
    },
    {
      "role": "user",
      "parts": [
        "great from now on write only the functions no other code\n{\n\"Func_name\": \"make_not_funny\",\n\"Implementation \": true,\n\"description\":\"This function takes a string input that is funny and modifies it to be not funny. Requires understanding of humor, making it better suited for human/NLP tools.\"\n}",
      ],
    },
    {
      "role": "model",
      "parts": [
        "```python\nimport os\nimport google.generativeai as genai\nimport json\nfrom google.ai.generativelanguage_v1beta.types import content\n\ngenai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n\ndef make_not_funny(funny_text: str) -> str:\n    \"\"\"\n    Takes a funny string and modifies it to be not funny.\n\n    Args:\n        funny_text: The input string that is considered funny.\n\n    Returns:\n        A modified string that is not funny, or an error message if the process fails.\n    \"\"\"\n    try:\n        generation_config = {\n            \"temperature\": 0.7,\n            \"top_p\": 0.95,\n            \"top_k\": 40,\n            \"max_output_tokens\": 8192,\n            \"response_schema\": content.Schema(\n                type=content.Type.OBJECT,\n                properties={\n                    \"not_funny_text\": content.Schema(\n                        type=content.Type.STRING,\n                    ),\n                },\n            ),\n            \"response_mime_type\": \"application/json\",\n        }\n\n        model = genai.GenerativeModel(\n            model_name=\"gemini-2.0-flash-exp\",\n            generation_config=generation_config,\n            system_instruction=\"You are an expert at making humorous text not funny.\",\n        )\n\n        chat_session = model.start_chat(\n            history=[],\n        )\n\n        prompt = f\"Modify the following text to make it not funny:\\n{funny_text}\"\n        response = chat_session.send_message(prompt)\n        \n        try:\n            json_output = json.loads(response.text)\n            return json_output.get(\"not_funny_text\", \"Error: Could not extract not funny text from json.\")\n        except json.JSONDecodeError:\n             return f\"Error: Could not decode json. The response is: {response.text}\"\n\n\n    except Exception as e:\n      return f\"An error occurred: {e}\"\n```\n",
      ],
    },
  ]
)

 response = chat_session.send_message(about)

 print(response.text)